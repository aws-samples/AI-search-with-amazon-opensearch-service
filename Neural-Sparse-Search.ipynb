{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59189659",
   "metadata": {},
   "source": [
    "# Neural Sparse Search with Amazon OpenSearch Service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7ada56",
   "metadata": {},
   "source": [
    "**Welcome to Neural sparse search notebook. Use this notebook to build a Search application powered by Amazon OpenSearch Service**\n",
    "\n",
    "In this notebook, you will perform the following steps in sequence,\n",
    "\n",
    "The lab includes the following steps:\n",
    "1. [Step 1: Get the Cloudformation outputs](#Step-1:-Get-the-Cloudformation-outputs)\n",
    "2. [Step 2: Create the OpenSearch-Sagemaker ML connector](#Step-2:-Create-the-OpenSearch-Sagemaker-ML-connector)\n",
    "3. [Step 3: Register and deploy the sparse encoding model in OpenSearch](#Step-3:-Register-and-deploy-the-sparse-encoding-model-in-OpenSearch)\n",
    "4. [Step 4: Create the OpenSearch ingest pipeline with sparse-encoding processor](#TODO-Step-4:-Create-the-OpenSearch-ingest-pipeline-with-sparse-encoding-processor)\n",
    "5. [Step 5: Create the opensearch index](#Step-5:-Create-the-opensearch-index)\n",
    "6. [Step 6: Prepare the dataset](#Step-6:-Prepare-the-dataset)\n",
    "7. [Step 7: Ingest the prepared data into OpenSearch](#Step-7:-Ingest-the-prepared-data-into-OpenSearch)\n",
    "8. [Step 8: Create two phase search pipeline](#Step-7:-create-twp-phase-search-pipeline)\n",
    "9. [Step 9: Launch the search application](#Step-7:-Launch-the-search-application)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94978bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install dependencies\n",
    "#Implement header-based authentication and request authentication for AWS services that support AWS auth v4\n",
    "%pip install requests_aws4auth\n",
    "#OpenSearch Python SDK\n",
    "%pip install opensearch_py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1beb0cb0",
   "metadata": {},
   "source": [
    "## Step 1: Get the Cloudformation outputs\n",
    "\n",
    "Here, we retrieve the services that are already deployed as a part of the cloudformation template to be used in building the application. The services include,\n",
    "1. **Sagemaker Endpoint**\n",
    "2. **OpenSearch Domain** Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b45f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker, boto3, json, time\n",
    "from sagemaker.session import Session\n",
    "import subprocess\n",
    "from IPython.utils import io\n",
    "\n",
    "cfn = boto3.client('cloudformation')\n",
    "\n",
    "response = cfn.list_stacks(StackStatusFilter=['CREATE_COMPLETE','UPDATE_COMPLETE'])\n",
    "\n",
    "for cfns in response['StackSummaries']:\n",
    "    if('TemplateDescription' in cfns.keys()):\n",
    "        if('Neural Sparse search' in cfns['TemplateDescription']):\n",
    "            stackname = cfns['StackName']\n",
    "stackname\n",
    "\n",
    "response = cfn.describe_stack_resources(\n",
    "    StackName=stackname\n",
    ")\n",
    "# for resource in response['StackResources']:\n",
    "#     if(resource['ResourceType'] == \"AWS::SageMaker::Endpoint\"):\n",
    "#         SagemakerEmbeddingEndpoint = resource['PhysicalResourceId']\n",
    "\n",
    "cfn_outputs = cfn.describe_stacks(StackName=stackname)['Stacks'][0]['Outputs']\n",
    "\n",
    "for output in cfn_outputs:\n",
    "    if('OpenSearchDomainEndpoint' in output['OutputKey']):\n",
    "        OpenSearchDomainEndpoint = output['OutputValue']\n",
    "        \n",
    "    if('SageMakerSparseModelEndpoint' in output['OutputKey']):\n",
    "        SagemakerEmbeddingEndpoint = output['OutputValue']\n",
    "    if('WebAppURL' in output['OutputKey']):\n",
    "        WebAppURL = output['OutputValue']\n",
    "        \n",
    "region = boto3.Session().region_name  \n",
    "        \n",
    "\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "\n",
    "\n",
    "\n",
    "print(\"stackname: \"+stackname)\n",
    "print(\"account_id: \"+account_id)  \n",
    "print(\"region: \"+region)\n",
    "print(\"SageMakerSparseModelEndpoint: \"+SagemakerEmbeddingEndpoint)\n",
    "print(\"OpenSearchDomainEndpoint: \"+OpenSearchDomainEndpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df146181",
   "metadata": {},
   "source": [
    "## Step 2: Initialise OpenSearch client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb0395c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialise OpenSearch client\n",
    "import boto3\n",
    "import requests \n",
    "from requests_aws4auth import AWS4Auth\n",
    "import json\n",
    "\n",
    "host = 'https://'+OpenSearchDomainEndpoint+'/'\n",
    "service = 'es'\n",
    "credentials = boto3.Session().get_credentials()\n",
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key, region, service, session_token=credentials.token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c006009",
   "metadata": {},
   "source": [
    "## Step 3: Create the OpenSearch-Sagemaker ML connector "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2865d7e1",
   "metadata": {},
   "source": [
    "Amazon OpenSearch Service AI connectors allows you to create a connector from OpenSearch Service to SageMaker Runtime.\n",
    "To create a connector, we use the Amazon OpenSearch Domain endpoint, SagemakerEndpoint that hosts the sparse encoding model and an IAM role that grants OpenSearch Service access to invoke the sagemaker model (this role is already created as a part of the cloudformation template)\n",
    "\n",
    "Here, Using the connector_id obtained from the previous step, we register and deploy the model in OpenSearch and get a model identifier (model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d82a8a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "connector_path_url = host+'_plugins/_ml/connectors/_create'\n",
    "register_deploy_model_path_url = host+'_plugins/_ml/models/_register?deploy=true'\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "#create connector\n",
    "payload_1 = {\n",
    "       \"name\": \"sparse encoding model\",\n",
    "       \"description\": \"Connector for sparse encoding model\",\n",
    "       \"version\": 1,\n",
    "       \"protocol\": \"aws_sigv4\",\n",
    "       \"credential\": {\n",
    "          \"roleArn\": \"arn:aws:iam::\"+account_id+\":role/opensearch-sagemaker-role\"\n",
    "       },\n",
    "       \"parameters\": {\n",
    "          \"region\": region,\n",
    "          \"service_name\": 'sagemaker'\n",
    "       },\n",
    "       \"actions\": [\n",
    "          {\n",
    "             \"action_type\": \"predict\",\n",
    "             \"method\": \"POST\",\n",
    "             \"headers\": {\n",
    "                \"content-type\": \"application/json\"\n",
    "             },\n",
    "             \"url\": \"https://runtime.sagemaker.\"+region+\".amazonaws.com/endpoints/\"+SagemakerEmbeddingEndpoint+\"/invocations\",\n",
    "             \"pre_process_function\": '\\n    StringBuilder builder = new StringBuilder();\\n    builder.append(\"\\\\\"\");\\n    builder.append(params.text_docs[0]);\\n    builder.append(\"\\\\\"\");\\n    def parameters = \"{\" +\"\\\\\"inputs\\\\\":\" + builder + \"}\";\\n    return \"{\" +\"\\\\\"parameters\\\\\":\" + parameters + \"}\";\\n    ', \n",
    "             \"request_body\": \"\"\"[\"${parameters.inputs}\"]\"\"\",\n",
    "          }\n",
    "       ]\n",
    "    }\n",
    "    \n",
    "\n",
    "r_1 = requests.post(connector_path_url, auth=awsauth, json=payload_1, headers=headers)\n",
    "connector_id = json.loads(r_1.text)[\"connector_id\"]\n",
    "time.sleep(1)\n",
    "\n",
    "#register and deploy model\n",
    "    \n",
    "payload_2 = { \n",
    "                \"name\": \"sparse encoding model\",\n",
    "                \"function_name\":\"remote\",\n",
    "                \"description\": \"sparse encoding model\",\n",
    "                \"connector_id\": connector_id\n",
    "                \n",
    "                }\n",
    "\n",
    "r_2 = requests.post(register_deploy_model_path_url, auth=awsauth, json=payload_2, headers=headers)\n",
    "model_id = json.loads(r_2.text)[\"model_id\"]\n",
    "time.sleep(1)\n",
    "    \n",
    "#test model\n",
    "\n",
    "payload_4 = {\n",
    "      \"parameters\": {\n",
    "        \"inputs\": \"hello\"\n",
    "          }\n",
    "            }\n",
    "\n",
    "path_4 = host+'_plugins/_ml/models/'+model_id+'/_predict'\n",
    "r_4 = requests.post(path_4, auth=awsauth, json=payload_4, headers=headers)\n",
    "print(r_4.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c4fa35",
   "metadata": {},
   "source": [
    "## Step 4: Create the OpenSearch ingest pipeline with sparse_encoding processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd49613",
   "metadata": {},
   "source": [
    "In the ingestion pipeline, you choose \"text_embedding\" processor to generate vector embeddings from \"caption\" field and store vector data in \"caption_embedding\" field of type knn_vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40babeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"_ingest/pipeline/sagemaker-sparse-ingest-pipeline\"\n",
    "url = host + path\n",
    "payload = {\n",
    "  \"description\": \"An sparse encoding ingest pipeline\",\n",
    "  \"processors\": [\n",
    "    {\n",
    "      \"sparse_encoding\": {\n",
    "        \"model_id\": model_id,\n",
    "        \"field_map\": {\n",
    "          \"product_description\": \"product_description_sparse_encoding\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "r = requests.put(url, auth=awsauth, json=payload, headers=headers)\n",
    "print(r.status_code)\n",
    "print(r.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e204695",
   "metadata": {},
   "source": [
    "## Step 5: Create the Sparse index with rank_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f21a9f",
   "metadata": {},
   "source": [
    "Create an opensearch index and set the pipeline created in the previous step \"sagemaker-sparse-ingest-pipeline\" as the default pipeline. The product_description_sparse_encoding field must be mapped as a reank_featuers field type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cb36eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"sagemaker-sparse-search-index\"\n",
    "url = host + path\n",
    "payload = {\n",
    "  \"settings\": {\n",
    "    \n",
    "    \"default_pipeline\": \"sagemaker-sparse-ingest-pipeline\",\n",
    "    \"number_of_shards\": 1,\n",
    "    \"number_of_replicas\": \"0\"\n",
    "  },\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"product_description_sparse_encoding\": {\n",
    "        \"type\": \"rank_features\"\n",
    "      },\n",
    "      \"product_description\": {\n",
    "        \"type\": \"text\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "r = requests.put(url, auth=awsauth, json=payload, headers=headers)\n",
    "print(r.status_code)\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3269d744",
   "metadata": {},
   "source": [
    "## Step 6: Download the dataset (.gz) and extract the .gz file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ed48ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import tarfile\n",
    "\n",
    "os.makedirs('tmp/images', exist_ok = True)\n",
    "metadata_file = urllib.request.urlretrieve('https://aws-blogs-artifacts-public.s3.amazonaws.com/BDB-3144/products-data.yml', 'tmp/images/products.yaml')\n",
    "img_filename,headers= urllib.request.urlretrieve('https://aws-blogs-artifacts-public.s3.amazonaws.com/BDB-3144/images.tar.gz', 'tmp/images/images.tar.gz')              \n",
    "print(img_filename)\n",
    "file = tarfile.open('tmp/images/images.tar.gz')\n",
    "file.extractall('tmp/images/')\n",
    "file.close()\n",
    "#remove images.tar.gz\n",
    "os.remove('tmp/images/images.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72bcc58",
   "metadata": {},
   "source": [
    "## Step 7: Ingest the prepared data into OpenSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fb015a",
   "metadata": {},
   "source": [
    "We ingest only the captions and the image urls of the images into the opensearch index\n",
    "\n",
    "This step takes approcimately 3 minutes to load the data into opensearch. \n",
    "\n",
    "**A total of 49 batches will be ingested into the index, where every batch has 50 documents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a649f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ruamel.yaml import YAML\n",
    "from PIL import Image\n",
    "import os\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "\n",
    "headers = { \"Content-Type\": \"application/json\"}\n",
    "aos_client = OpenSearch(\n",
    "    hosts = [{'host': OpenSearchDomainEndpoint, 'port': 443}],\n",
    "    http_auth = awsauth,\n",
    "    use_ssl = True,\n",
    "    #verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection\n",
    ")\n",
    "\n",
    "# Load the products from the dataset\n",
    "yaml = YAML()\n",
    "items_ = yaml.load(open('tmp/images/products.yaml'))\n",
    "\n",
    "batch = 0\n",
    "count = 0\n",
    "body_ = ''\n",
    "batch_size = 50\n",
    "last_batch = int(len(items_)/batch_size)\n",
    "action = json.dumps({ 'index': { '_index': 'sagemaker-sparse-search-index' } })\n",
    "\n",
    "for item in items_:\n",
    "    count+=1\n",
    "    payload = {}\n",
    "    payload['image_url'] = \"/home/ec2-user/SageMaker/AI-search-with-amazon-opensearch-service/tmp/images/\"+item[\"category\"]+\"/\"+item[\"image\"]\n",
    "    payload['product_description'] = item['description']\n",
    "    payload['caption'] = item['name']\n",
    "    payload['category'] = item['category']\n",
    "    payload['price'] = item['price']\n",
    "    if('gender_affinity' in item):\n",
    "        if(item['gender_affinity'] == 'M'):\n",
    "            payload['gender_affinity'] = 'Male'\n",
    "        else:\n",
    "            if(item['gender_affinity'] == 'F'):\n",
    "                payload['gender_affinity'] = 'Female'\n",
    "            else:\n",
    "                payload['gender_affinity'] = item['gender_affinity']\n",
    "    if('style' in item):          \n",
    "        payload['style'] = item['style']\n",
    "    \n",
    "    body_ = body_ + action + \"\\n\" + json.dumps(payload) + \"\\n\"\n",
    "    \n",
    "    if(count == batch_size):\n",
    "        response = aos_client.bulk(\n",
    "        index = 'sagemaker-sparse-search-index',\n",
    "        body = body_\n",
    "        )\n",
    "        batch += 1\n",
    "        count = 0\n",
    "        print(\"batch \"+str(batch) + \" ingestion done!\")\n",
    "        if(batch != last_batch):\n",
    "            body_ = \"\"\n",
    "        \n",
    "            \n",
    "#ingest the remaining rows\n",
    "response = aos_client.bulk(\n",
    "        index = 'sagemaker-sparse-search-index',\n",
    "        body = body_\n",
    "        )\n",
    "        \n",
    "print(\"All \"+str(last_batch)+\" batches ingested into index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b5784b",
   "metadata": {},
   "source": [
    "## Step 8: Create two phase search pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c53c2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"_search/pipeline/neural_sparse_two_phase_search_pipeline\"\n",
    "url = host + path\n",
    "payload = {\n",
    "  \"request_processors\": [\n",
    "    {\n",
    "      \"neural_sparse_two_phase_processor\": {\n",
    "        \"tag\": \"neural-sparse\",\n",
    "        \"description\": \"This processor is making two-phase processor.\",\n",
    "        \"enabled\": True,\n",
    "        \"two_phase_parameter\": {\n",
    "          \"prune_ratio\": 0.4\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "r = requests.put(url, auth=awsauth, json=payload, headers=headers)\n",
    "print(r.status_code)\n",
    "print(r.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1154fad",
   "metadata": {},
   "source": [
    "## Step 9: Launch the Search application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddb056a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(WebAppURL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
