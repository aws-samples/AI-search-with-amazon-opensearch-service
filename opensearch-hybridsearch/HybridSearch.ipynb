{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59189659",
   "metadata": {},
   "source": [
    "# Hybrid Search with Amazon OpenSearch Service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954e8f9b",
   "metadata": {},
   "source": [
    "**Welcome to Hybrid search notebook. Use this notebook to build a Hybrid Search application powered by Amazon OpenSearch Service**\n",
    "\n",
    "In this notebook, you will perform the following steps in sequence,\n",
    "\n",
    "The lab includes the following steps:\n",
    "1. [Step 1: Get the Cloudformation outputs](#Step-1:-Get-the-Cloudformation-outputs)\n",
    "2. [Step 2: Create the OpenSearch-Sagemaker ML connector](#Step-2:-Create-the-OpenSearch-Sagemaker-ML-connector)\n",
    "3. [Step 3: Register and deploy the embedding model in OpenSearch](#Step-3:-Register-and-deploy-the-embedding-model-in-OpenSearch)\n",
    "4. [Step 4: Create the OpenSearch ingest pipeline with text-embedding processor](#TODO-Step-4:-Create-the-OpenSearch-ingest-pipeline-with-text-embedding-processor)\n",
    "5. [Step 5: Create the k-NN index](#Step-5:-Create-the-k-NN-index)\n",
    "6. [Step 6: Prepare the image dataset](#Step-6:-Prepare-the-image-dataset)\n",
    "7. [Step 7: Ingest the prepared data into OpenSearch](#Step-7:-Ingest-the-prepared-data-into-OpenSearch)\n",
    "8. [Step 8: Update the environment variables of lambda](#Step-8:-Update-the-environment-variables-of-lambda)\n",
    "9. [Step 9: Create the Lambda URL](#Step-9:-Create-the-Lambda-URL)\n",
    "10. [Step 10: Host the Hybrid Search application in EC2](#Step-7:-Host-the-Hybrid-Search-application-in-EC2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94978bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install dependencies\n",
    "#Implement header-based authentication and request authentication for AWS services that support AWS auth v4\n",
    "%pip install requests_aws4auth\n",
    "#OpenSearch Python SDK\n",
    "%pip install opensearch_py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1beb0cb0",
   "metadata": {},
   "source": [
    "## Step 1: Get the Cloudformation outputs\n",
    "\n",
    "Here, we retrieve the services that are already deployed as a part of the cloudformation template to be used in building the application. The services include,\n",
    "1. **Sagemaker Endpoint**\n",
    "2. **OpenSearch Domain** Endpoint\n",
    "3. **S3** Bucket name\n",
    "4. **Lambda** Function name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b45f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker, boto3, json, time\n",
    "from sagemaker.session import Session\n",
    "import subprocess\n",
    "from IPython.utils import io\n",
    "\n",
    "cfn = boto3.client('cloudformation')\n",
    "\n",
    "response = cfn.list_stacks(StackStatusFilter=['CREATE_COMPLETE','UPDATE_COMPLETE'])\n",
    "\n",
    "for cfns in response['StackSummaries']:\n",
    "    if('TemplateDescription' in cfns.keys()):\n",
    "        if('hybrid search' in cfns['TemplateDescription']):\n",
    "            stackname = cfns['StackName']\n",
    "stackname\n",
    "\n",
    "response = cfn.describe_stack_resources(\n",
    "    StackName=stackname\n",
    ")\n",
    "# for resource in response['StackResources']:\n",
    "#     if(resource['ResourceType'] == \"AWS::SageMaker::Endpoint\"):\n",
    "#         SagemakerEmbeddingEndpoint = resource['PhysicalResourceId']\n",
    "\n",
    "cfn_outputs = cfn.describe_stacks(StackName=stackname)['Stacks'][0]['Outputs']\n",
    "\n",
    "for output in cfn_outputs:\n",
    "    if('OpenSearchDomainEndpoint' in output['OutputKey']):\n",
    "        OpenSearchDomainEndpoint = output['OutputValue']\n",
    "        \n",
    "    if('EmbeddingEndpointName' in output['OutputKey']):\n",
    "        SagemakerEmbeddingEndpoint = output['OutputValue']\n",
    "        \n",
    "    if('s3' in output['OutputKey'].lower()):\n",
    "        s3_bucket = output['OutputValue']\n",
    "        \n",
    "    if('lambdafunction' in output['OutputKey'].lower()):\n",
    "        lambdaFunction = output['OutputValue']\n",
    "\n",
    "region = boto3.Session().region_name  \n",
    "        \n",
    "\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "\n",
    "\n",
    "\n",
    "print(\"stackname: \"+stackname)\n",
    "print(\"account_id: \"+account_id)  \n",
    "print(\"region: \"+region)\n",
    "print(\"SagemakerEmbeddingEndpoint: \"+SagemakerEmbeddingEndpoint)\n",
    "print(\"OpenSearchDomainEndpoint: \"+OpenSearchDomainEndpoint)\n",
    "print(\"S3 Bucket: \"+s3_bucket)\n",
    "print(\"lambda Function : \"+lambdaFunction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c006009",
   "metadata": {},
   "source": [
    "## Step 2: Create the OpenSearch-Sagemaker ML connector "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d252adc",
   "metadata": {},
   "source": [
    "Amazon OpenSearch Service AI connectors allows you to create a connector from OpenSearch Service to SageMaker Runtime.\n",
    "To create a connector, we use the Amazon OpenSearch Domain endpoint, SagemakerEndpoint that hosts the GPT-J-6B embedding model and an IAM role that grants OpenSearch Service access to invoke the sagemaker model (this role is already created as a part of the cloudformation template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb0395c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import requests \n",
    "from requests_aws4auth import AWS4Auth\n",
    "import json\n",
    "\n",
    "host = 'https://'+OpenSearchDomainEndpoint+'/'\n",
    "service = 'es'\n",
    "credentials = boto3.Session().get_credentials()\n",
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key, region, service, session_token=credentials.token)\n",
    "\n",
    "# Register repository\n",
    "path = '_plugins/_ml/connectors/_create'\n",
    "url = host + path\n",
    "\n",
    "payload = {\n",
    "   \"name\": \"sagemaker: embedding\",\n",
    "   \"description\": \"Test connector for Sagemaker embedding model\",\n",
    "   \"version\": 1,\n",
    "   \"protocol\": \"aws_sigv4\",\n",
    "   \"credential\": {\n",
    "      \"roleArn\": \"arn:aws:iam::\"+account_id+\":role/opensearch-sagemaker-role\"\n",
    "   },\n",
    "   \"parameters\": {\n",
    "      \"region\": region,\n",
    "      \"service_name\": \"sagemaker\"\n",
    "   },\n",
    "   \"actions\": [\n",
    "      {\n",
    "         \"action_type\": \"predict\",\n",
    "         \"method\": \"POST\",\n",
    "         \"headers\": {\n",
    "            \"content-type\": \"application/json\"\n",
    "         },\n",
    "         \"url\": \"https://runtime.sagemaker.\"+region+\".amazonaws.com/endpoints/\"+SagemakerEmbeddingEndpoint+\"/invocations\",\n",
    "     \"pre_process_function\": '\\n    StringBuilder builder = new StringBuilder();\\n    builder.append(\"\\\\\"\");\\n    builder.append(params.text_docs[0]);\\n    builder.append(\"\\\\\"\");\\n    def parameters = \"{\" +\"\\\\\"inputs\\\\\":\" + builder + \"}\";\\n    return \"{\" +\"\\\\\"parameters\\\\\":\" + parameters + \"}\";\\n    ', \n",
    "         \"request_body\": \"{ \\\"text_inputs\\\": \\\"${parameters.inputs}\\\"}\",\n",
    "         \"post_process_function\": '\\n    def name = \"sentence_embedding\";\\n    def dataType = \"FLOAT32\";\\n    if (params.embedding == null || params.embedding.length == 0) {\\n        return null;\\n    }\\n    def shape = [params.embedding[0].length];\\n    def json = \"{\" +\\n               \"\\\\\"name\\\\\":\\\\\"\" + name + \"\\\\\",\" +\\n               \"\\\\\"data_type\\\\\":\\\\\"\" + dataType + \"\\\\\",\" +\\n               \"\\\\\"shape\\\\\":\" + shape + \",\" +\\n               \"\\\\\"data\\\\\":\" + params.embedding[0] +\\n               \"}\";\\n    return json;\\n    '\n",
    "}\n",
    "   ]\n",
    "}\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "r = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "print(r.status_code)\n",
    "print(r.text)\n",
    "connector_id = json.loads(r.text)[\"connector_id\"]\n",
    "connector_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accdb849",
   "metadata": {},
   "source": [
    "## Step 3: Register and deploy the embedding model in OpenSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac80c994",
   "metadata": {},
   "source": [
    "Here, Using the connector_id obtained from the previous step, we register and deploy the model in OpenSearch and get a model identifier (model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81ed066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the model\n",
    "path = '_plugins/_ml/models/_register'\n",
    "url = host + path\n",
    "\n",
    "payload = { \"name\": \"embedding-gpt\",\n",
    "    \"function_name\": \"remote\",\n",
    "    \"description\": \"embeddings model\",\n",
    "    \"connector_id\": connector_id}\n",
    "\n",
    "r = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "model_id = json.loads(r.text)[\"model_id\"]\n",
    "print(\"Model registered under model_id: \"+model_id)\n",
    "\n",
    "# Deploy the model\n",
    "\n",
    "path = '_plugins/_ml/models/'+model_id+'/_deploy'\n",
    "url = host + path\n",
    "\n",
    "r = requests.post(url, auth=awsauth, headers=headers)\n",
    "deploy_status = json.loads(r.text)[\"status\"]\n",
    "\n",
    "print(\"Deployment status of the model, \"+model_id+\" : \"+deploy_status)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e264883",
   "metadata": {},
   "source": [
    "## (Optional) Test the embedding model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ec669a",
   "metadata": {},
   "source": [
    "Optional: run this snippet to test that the OpenSearch-Sagemaker connection is successful and you can generate an embedding for text. These embeddings produced by the GPT-J-6B model are 4096 dimensional, here, we print  the first five embedding dimensional values of a sample text \"hello\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c31f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '_plugins/_ml/models/'+model_id+'/_predict'\n",
    "url = host + path\n",
    "\n",
    "payload = {\n",
    "  \"parameters\": {\n",
    "    \"inputs\": \"hello\"\n",
    "  }\n",
    "}\n",
    "r = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "embed = json.loads(r.text)#[\"inference_results\"][0][\"output\"][0][\"data\"][0:5]\n",
    "print(embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c4fa35",
   "metadata": {},
   "source": [
    "## Step 4: Create the OpenSearch ingest pipeline with text-embedding processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd49613",
   "metadata": {},
   "source": [
    "In the ingestion pipeline, you choose \"text_embedding\" processor to generate vector embeddings from \"caption\" field and store vector data in \"caption_embedding\" field of type knn_vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef0ab48",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"_ingest/pipeline/nlp-ingest-pipeline\"\n",
    "url = host + path\n",
    "payload = {\n",
    "  \"description\": \"An NLP ingest pipeline\",\n",
    "  \"processors\": [\n",
    "    {\n",
    "      \"text_embedding\": {\n",
    "        \"model_id\": model_id,\n",
    "        \"field_map\": {\n",
    "          \"caption\": \"caption_embedding\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "r = requests.put(url, auth=awsauth, json=payload, headers=headers)\n",
    "print(r.status_code)\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e204695",
   "metadata": {},
   "source": [
    "## Step 5: Create the k-NN index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f21a9f",
   "metadata": {},
   "source": [
    "Create the K-NN index and set the pipeline created in the previous step \"nlp-ingest-pipeline\" as the default pipeline. The caption_embedding field must be mapped as a k-NN vector with 4096 dimensions matching the model dimension. \n",
    "\n",
    "For the kNN index we use **nmslib** engine with **hnsw** algorithm and **l2** spacetype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368d0c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"nlp-image-search-index\"\n",
    "url = host + path\n",
    "payload = {\n",
    "  \"settings\": {\n",
    "    \"index.knn\": True,\n",
    "    \"default_pipeline\": \"nlp-ingest-pipeline\"\n",
    "  },\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"image_s3_url\": {\n",
    "        \"type\": \"text\"\n",
    "      },\n",
    "      \"caption_embedding\": {\n",
    "        \"type\": \"knn_vector\",\n",
    "        \"dimension\": 4096,\n",
    "        \"method\": {\n",
    "          \"engine\": \"nmslib\",\n",
    "          \"space_type\": \"l2\",\n",
    "          \"name\": \"hnsw\",\n",
    "          \"parameters\": {}\n",
    "        }\n",
    "      },\n",
    "      \"caption\": {\n",
    "        \"type\": \"text\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "r = requests.put(url, auth=awsauth, json=payload, headers=headers)\n",
    "print(r.status_code)\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3269d744",
   "metadata": {},
   "source": [
    "## Step 6: Prepare the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9815d45",
   "metadata": {},
   "source": [
    "Download the Amazon Bekerley dataset from S3 and pre-process in such a way that you get the image properties in a dataframe\n",
    "\n",
    "For simplicity we use only 1655 sample images from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef813f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "meta = pd.read_json(\"s3://amazon-berkeley-objects/listings/metadata/listings_0.json.gz\", lines=True)\n",
    "def func_(x):\n",
    "    us_texts = [item[\"value\"] for item in x if item[\"language_tag\"] == \"en_US\"]\n",
    "    return us_texts[0] if us_texts else None\n",
    " \n",
    "meta = meta.assign(item_name_in_en_us=meta.item_name.apply(func_))\n",
    "meta = meta[~meta.item_name_in_en_us.isna()][[\"item_id\", \"item_name_in_en_us\", \"main_image_id\"]]\n",
    "print(f\"#products with US English title: {len(meta)}\")\n",
    "meta.head()\n",
    "\n",
    "image_meta = pd.read_csv(\"s3://amazon-berkeley-objects/images/metadata/images.csv.gz\")\n",
    "dataset = meta.merge(image_meta, left_on=\"main_image_id\", right_on=\"image_id\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72bcc58",
   "metadata": {},
   "source": [
    "## Step 7: Ingest the prepared data into OpenSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fb015a",
   "metadata": {},
   "source": [
    "We ingest only the captions and the image urls of the images into the opensearch index\n",
    "\n",
    "This step takes approcimately 10 minutes to load the data into opensearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39156cd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = \"nlp-image-search-index/_doc\" \n",
    "url = host + path\n",
    "\n",
    "cnt = 0\n",
    "for index, row in dataset.iterrows():\n",
    "    payload = {}\n",
    "    payload['image_s3_url'] = \"https://amazon-berkeley-objects.s3.amazonaws.com/images/small/\"+row['path']\n",
    "    payload['caption'] = row['item_name_in_en_us']\n",
    "    \n",
    "    r = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "    cnt = cnt+1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8520d9a1",
   "metadata": {},
   "source": [
    "### The following 2 steps are optional because in the final web application, these steps are performed by the Lambda function itself that is already deployed by the cloud formation template.\n",
    "## Create the Search pipeline in OpenSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85ff601",
   "metadata": {},
   "source": [
    "Create a search pipeline in OpenSearch to normalize the search results from the text and vector search queries. The search pipeline combines the results from each subquery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32799da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"_search/pipeline/nlp-search-pipeline\" \n",
    "url = host + path\n",
    "\n",
    "payload = {\n",
    "  \"description\": \"Post processor for hybrid search\",\n",
    "  \"phase_results_processors\": [\n",
    "    {\n",
    "      \"normalization-processor\": {\n",
    "        \"normalization\": {\n",
    "          \"technique\": \"min_max\"\n",
    "        },\n",
    "        \"combination\": {\n",
    "          \"technique\": \"arithmetic_mean\",\n",
    "          \"parameters\": {\n",
    "            \"weights\": [\n",
    "              0.3,\n",
    "              0.7\n",
    "            ]\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "\n",
    "r = requests.put(url, auth=awsauth, json=payload, headers=headers)\n",
    "print(r.status_code)\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04fc3f2",
   "metadata": {},
   "source": [
    "## Search with Hybrid Query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8af4dfd",
   "metadata": {},
   "source": [
    "This is an example of Hybrid query that you will run uing the web application later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418d2629",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"nlp-image-search-index/_search?search_pipeline=nlp-search-pipeline\" \n",
    "url = host + path\n",
    "query_ = \"wine glass\"\n",
    "\n",
    "payload = {\n",
    "  \"_source\": {\n",
    "    \"exclude\": [\n",
    "      \"caption_embedding\"\n",
    "    ]\n",
    "  },\n",
    "  \"query\": {\n",
    "    \"hybrid\": {\n",
    "      \"queries\": [\n",
    "        {\n",
    "          \"match\": {\n",
    "            \"caption\": {\n",
    "              \"query\": query_\n",
    "            }\n",
    "          }\n",
    "        },\n",
    "        {\n",
    "          \"neural\": {\n",
    "            \"caption_embedding\": {\n",
    "              \"query_text\": query_,\n",
    "              \"model_id\": model_id,\n",
    "              \"k\": 2\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  },\"size\":1\n",
    "}\n",
    "\n",
    "r = requests.get(url, auth=awsauth, json=payload, headers=headers)\n",
    "print(r.status_code)\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f39448",
   "metadata": {},
   "source": [
    "## Step 8: Update the environment variables of lambda\n",
    "\n",
    "Here, we pass the OpenSearch endpoint, AWS region and OpenSearch model identifier to Lambda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c557766",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_client = boto3.client('lambda')\n",
    "\n",
    "response = lambda_client.update_function_configuration(\n",
    "            FunctionName=lambdaFunction,\n",
    "            Environment={\n",
    "                'Variables': {\n",
    "                    'DOMAIN_ENDPOINT': OpenSearchDomainEndpoint,\n",
    "                    'REGION':region,\n",
    "                    'MODEL_ID':model_id\n",
    "                }\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18947159",
   "metadata": {},
   "source": [
    "## Step 9: Create the Lambda URL\n",
    "\n",
    "Here we create external Lambda URL for lambda function to be called from the outside world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a97269a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ = boto3.client('lambda')\n",
    "\n",
    "\n",
    "response_ = lambda_.add_permission(\n",
    "FunctionName=lambdaFunction,\n",
    "StatementId=lambdaFunction+'_permissions',\n",
    "Action=\"lambda:InvokeFunctionUrl\",\n",
    "Principal=account_id,\n",
    "FunctionUrlAuthType='AWS_IAM')\n",
    "\n",
    "\n",
    "response = lambda_.create_function_url_config(\n",
    "FunctionName=lambdaFunction,\n",
    "AuthType='AWS_IAM',\n",
    "Cors={\n",
    "    'AllowCredentials': True,\n",
    "\n",
    "    'AllowMethods':[\"*\"],\n",
    "    'AllowOrigins': [\"*\"]\n",
    "\n",
    "},\n",
    "InvokeMode='RESPONSE_STREAM'\n",
    ")\n",
    "\n",
    "query_invoke_URL = response['FunctionUrl']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1154fad",
   "metadata": {},
   "source": [
    "## Step 10: Host the Hybrid Search application in EC2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b857dea",
   "metadata": {},
   "source": [
    "## Notice\n",
    "\n",
    "To ensure security access to the provisioned resources, we use EC2 security group to limit access scope. Before you go into the final step, you need to add your current **PUBLIC IP** address to the ec2 security group so that you are able to access the web application (chat interface) that you are going to host in the next step.\n",
    "\n",
    "<h3 style=\"color:red;\"><U>Warning</U></h3>\n",
    "<h4>Without doing the below steps, you will not be able to proceed further.</h4>\n",
    "\n",
    "<div>\n",
    "    <h3 style=\"color:red;\"><U>Enter your IP address </U></h3>\n",
    "    <h4> STEP 1. Get your IP address <span style=\"display:inline;color:blue\"><a href = \"https://ipinfo.io/ip \">HERE</a></span>. If you are connecting with VPN, we recommend you disconnect VPN first.</h4>\n",
    "</div>\n",
    "\n",
    "<h4>STEP 2. Run the below cell </h4>\n",
    "<h4>STEP 3. Paste the IP address in the input box that prompts you to enter your IP</h4>\n",
    "<h4>STEP 4. Press ENTER</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddb056a",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_ip = (input(\"Enter your IP : \")).split(\".\")\n",
    "my_ip.pop()\n",
    "IP = \".\".join(my_ip)+\".0/24\"\n",
    "\n",
    "port_protocol = {443:'HTTPS',80:'HTTP',8501:'streamlit'}\n",
    "\n",
    "IpPermissions = []\n",
    "\n",
    "for port in port_protocol.keys():\n",
    "     IpPermissions.append({\n",
    "            'FromPort': port,\n",
    "            'IpProtocol': 'tcp',\n",
    "            'IpRanges': [\n",
    "                {\n",
    "                    'CidrIp': IP,\n",
    "                    'Description': port_protocol[port]+' access',\n",
    "                },\n",
    "            ],\n",
    "            'ToPort': port,\n",
    "        })\n",
    "\n",
    "IpPermissions\n",
    "\n",
    "for output in cfn_outputs:\n",
    "    if('securitygroupid' in output['OutputKey'].lower()):\n",
    "        sg_id = output['OutputValue']\n",
    "        \n",
    "#sg_id = 'sg-0e0d72baa90696638'\n",
    "\n",
    "ec2_ = boto3.client('ec2')        \n",
    "\n",
    "response = ec2_.authorize_security_group_ingress(\n",
    "    GroupId=sg_id,\n",
    "    IpPermissions=IpPermissions,\n",
    ")\n",
    "\n",
    "print(\"\\nIngress rules added for the security group, ports:protocol - \"+json.dumps(port_protocol)+\" with my ip - \"+IP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047e4328",
   "metadata": {},
   "source": [
    "Finally, We are ready to host our conversational search application, here we perform the following steps, Steps 2-5 are achieved by executing the terminal commands in the ec2 instance using a SSM client.\n",
    "1. Update the web application code files with lambda url (in [api.py](https://github.com/aws-samples/semantic-search-with-amazon-opensearch/blob/main/generative-ai/Module_1_Build_Conversational_Search/webapp/api.py)) and s3 bucket name (in [app.py](https://github.com/aws-samples/semantic-search-with-amazon-opensearch/blob/main/generative-ai/Module_1_Build_Conversational_Search/webapp/app.py))\n",
    "2. Archieve the application files and push to the configured s3 bucket.\n",
    "3. Download the application (.zip) from s3 bucket into ec2 instance (/home/ec2-user/), and uncompress it.\n",
    "4. We install the streamlit and boto3 dependencies inside a virtual environment inside the ec2 instance.\n",
    "5. Start the streamlit application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc79d845",
   "metadata": {},
   "outputs": [],
   "source": [
    "#modify the code files with lambda url and s3 bucket names\n",
    "query_invoke_URL_cmd = query_invoke_URL.replace(\"/\",\"\\/\")\n",
    "\n",
    "with io.capture_output() as captured:\n",
    "    #Update the webapp files to include the s3 bucket name and the LambdaURL\n",
    "    !sed -i 's/API_URL_TO_BE_REPLACED/{query_invoke_URL_cmd}/g' webapp/api.py\n",
    "    #Push the WebAPP code artefacts to s3\n",
    "    !cd webapp && zip -r webapp.zip *\n",
    "    !aws s3 cp webapp/webapp.zip s3://$s3_bucket\n",
    "        \n",
    "#Get the Ec2 instance ID which is already deployed\n",
    "response = cfn.describe_stack_resources(\n",
    "    StackName=stackname\n",
    ")\n",
    "for resource in response['StackResources']:\n",
    "    if(resource['ResourceType'] == 'AWS::EC2::Instance'):\n",
    "        ec2_instance_id = resource['PhysicalResourceId']\n",
    "   \n",
    "ec2_instance_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b1351e",
   "metadata": {},
   "source": [
    "Copy the URL that will be generated after running the next cell and open the URL in your web browser to start using the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e2339b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to execute commands in ec2 terminal\n",
    "def execute_commands_on_linux_instances(client, commands):\n",
    "    resp = client.send_command(\n",
    "        DocumentName=\"AWS-RunShellScript\", # One of AWS' preconfigured documents\n",
    "        Parameters={'commands': commands},\n",
    "        InstanceIds=[ec2_instance_id],\n",
    "    )\n",
    "    return resp['Command']['CommandId']\n",
    "\n",
    "ssm_client = boto3.client('ssm') \n",
    "\n",
    "commands = [\n",
    "            'aws s3 cp s3://'+s3_bucket+'/webapp.zip /home/ec2-user/',\n",
    "            'unzip -o /home/ec2-user/webapp.zip -d /home/ec2-user/'  ,  \n",
    "            'sudo chmod -R 0777 /home/ec2-user/',\n",
    "            'python3 -m venv /home/ec2-user/.myenv',\n",
    "            'source /home/ec2-user/.myenv/bin/activate',\n",
    "            'pip install streamlit',\n",
    "            'pip install boto3',\n",
    "    \n",
    "            #start the web applicaiton\n",
    "            'streamlit run /home/ec2-user/app.py',\n",
    "            ]\n",
    "\n",
    "command_id = execute_commands_on_linux_instances(ssm_client, commands)\n",
    "\n",
    "ec2_ = boto3.client('ec2')\n",
    "response = ec2_.describe_instances(\n",
    "    InstanceIds=[ec2_instance_id]\n",
    ")\n",
    "public_ip = response['Reservations'][0]['Instances'][0]['PublicIpAddress']\n",
    "print(\"Please wait while the application is being hosted . . .\")\n",
    "time.sleep(10)\n",
    "print(\"\\nApplication hosted successfully\")\n",
    "print(\"\\nClick the below URL to open the application. It may take up to a minute or two to start the application, Please keep refreshing the page if you are seeing connection error.\\n\")\n",
    "print('http://'+public_ip+\":8501\")\n",
    "#print(\"\\nCheck the below video on how to interact with the application\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
