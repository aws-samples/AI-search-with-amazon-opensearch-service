{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59189659",
   "metadata": {},
   "source": [
    "# Hybrid Search with Amazon OpenSearch Service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954e8f9b",
   "metadata": {},
   "source": [
    "**Welcome to Hybrid search notebook. Use this notebook to build a Hybrid Search application powered by Amazon OpenSearch Service**\n",
    "\n",
    "In this notebook, you will perform the following steps in sequence,\n",
    "\n",
    "The lab includes the following steps:\n",
    "1. [Step 1: Get the Cloudformation outputs](#Step-1:-Get-the-Cloudformation-outputs)\n",
    "2. [Step 2: Create the OpenSearch-Sagemaker ML connector](#Step-2:-Create-the-OpenSearch-Sagemaker-ML-connector)\n",
    "3. [Step 3: Register and deploy the embedding model in OpenSearch](#Step-3:-Register-and-deploy-the-embedding-model-in-OpenSearch)\n",
    "4. [Step 4: Create the OpenSearch ingest pipeline with text-embedding processor](#TODO-Step-4:-Create-the-OpenSearch-ingest-pipeline-with-text-embedding-processor)\n",
    "5. [Step 5: Create the k-NN index](#Step-5:-Create-the-k-NN-index)\n",
    "6. [Step 6: Prepare the image dataset](#Step-6:-Prepare-the-image-dataset)\n",
    "7. [Step 7: Ingest the prepared data into OpenSearch](#Step-7:-Ingest-the-prepared-data-into-OpenSearch)\n",
    "8. [Step 8: Update the environment variables of lambda](#Step-8:-Update-the-environment-variables-of-lambda)\n",
    "9. [Step 9: Create the Lambda URL](#Step-9:-Create-the-Lambda-URL)\n",
    "10. [Step 10: Host the Hybrid Search application in EC2](#Step-7:-Host-the-Hybrid-Search-application-in-EC2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94978bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install dependencies\n",
    "#Implement header-based authentication and request authentication for AWS services that support AWS auth v4\n",
    "%pip install requests_aws4auth\n",
    "#OpenSearch Python SDK\n",
    "%pip install opensearch_py\n",
    "#Progress bar for for loop\n",
    "%pip install alive-progress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1beb0cb0",
   "metadata": {},
   "source": [
    "## Step 1: Get the Cloudformation outputs\n",
    "\n",
    "Here, we retrieve the services that are already deployed as a part of the cloudformation template to be used in building the application. The services include,\n",
    "1. **Sagemaker Endpoint**\n",
    "2. **OpenSearch Domain** Endpoint\n",
    "3. **S3** Bucket name\n",
    "4. **Lambda** Function name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b45f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker, boto3, json, time\n",
    "from sagemaker.session import Session\n",
    "import subprocess\n",
    "from IPython.utils import io\n",
    "\n",
    "cfn = boto3.client('cloudformation')\n",
    "\n",
    "response = cfn.list_stacks(StackStatusFilter=['CREATE_COMPLETE','UPDATE_COMPLETE'])\n",
    "\n",
    "for cfns in response['StackSummaries']:\n",
    "    if('TemplateDescription' in cfns.keys()):\n",
    "        if('hybrid search' in cfns['TemplateDescription']):\n",
    "            stackname = cfns['StackName']\n",
    "stackname\n",
    "\n",
    "response = cfn.describe_stack_resources(\n",
    "    StackName=stackname\n",
    ")\n",
    "# for resource in response['StackResources']:\n",
    "#     if(resource['ResourceType'] == \"AWS::SageMaker::Endpoint\"):\n",
    "#         SagemakerEmbeddingEndpoint = resource['PhysicalResourceId']\n",
    "\n",
    "cfn_outputs = cfn.describe_stacks(StackName=stackname)['Stacks'][0]['Outputs']\n",
    "\n",
    "for output in cfn_outputs:\n",
    "    if('OpenSearchDomainEndpoint' in output['OutputKey']):\n",
    "        OpenSearchDomainEndpoint = output['OutputValue']\n",
    "        \n",
    "    if('EmbeddingEndpointName' in output['OutputKey']):\n",
    "        SagemakerEmbeddingEndpoint = output['OutputValue']\n",
    "        \n",
    "    if('s3' in output['OutputKey'].lower()):\n",
    "        s3_bucket = output['OutputValue']\n",
    "        \n",
    "    if('lambdafunction' in output['OutputKey'].lower()):\n",
    "        lambdaFunction = output['OutputValue']\n",
    "\n",
    "region = boto3.Session().region_name  \n",
    "        \n",
    "\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "\n",
    "\n",
    "\n",
    "print(\"stackname: \"+stackname)\n",
    "print(\"account_id: \"+account_id)  \n",
    "print(\"region: \"+region)\n",
    "print(\"SagemakerEmbeddingEndpoint: \"+SagemakerEmbeddingEndpoint)\n",
    "print(\"OpenSearchDomainEndpoint: \"+OpenSearchDomainEndpoint)\n",
    "print(\"S3 Bucket: \"+s3_bucket)\n",
    "print(\"lambda Function : \"+lambdaFunction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c006009",
   "metadata": {},
   "source": [
    "## Step 2: Create the OpenSearch-Sagemaker ML connector "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d252adc",
   "metadata": {},
   "source": [
    "Amazon OpenSearch Service AI connectors allows you to create a connector from OpenSearch Service to SageMaker Runtime.\n",
    "To create a connector, we use the Amazon OpenSearch Domain endpoint, SagemakerEndpoint that hosts the GPT-J-6B embedding model and an IAM role that grants OpenSearch Service access to invoke the sagemaker model (this role is already created as a part of the cloudformation template)\n",
    "\n",
    "Here, Using the connector_id obtained from the previous step, we register and deploy the model in OpenSearch and get a model identifier (model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb0395c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import requests \n",
    "from requests_aws4auth import AWS4Auth\n",
    "import json\n",
    "\n",
    "host = 'https://'+OpenSearchDomainEndpoint+'/'\n",
    "service = 'es'\n",
    "credentials = boto3.Session().get_credentials()\n",
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key, region, service, session_token=credentials.token)\n",
    "\n",
    "\n",
    "remote_ml = {\n",
    "                \"sagemaker_sparse\":\n",
    "                 {\n",
    "                     \"endpoint_url\":\"https://runtime.sagemaker.us-east-1.amazonaws.com/endpoints/sparse-os-search-os/invocations\",\n",
    "                     \"pre_process_fun\": '\\n    StringBuilder builder = new StringBuilder();\\n    builder.append(\"\\\\\"\");\\n    builder.append(params.text_docs[0]);\\n    builder.append(\"\\\\\"\");\\n    def parameters = \"{\" +\"\\\\\"inputs\\\\\":\" + builder + \"}\";\\n    return \"{\" +\"\\\\\"parameters\\\\\":\" + parameters + \"}\";\\n    ', \n",
    "                    \"post_process_fun\": '\\n    def name = \"sentence_embedding\";\\n    def dataType = \"FLOAT32\";\\n    if (params.result == null || params.result.length == 0) {\\n        return null;\\n    }\\n    def shape = [params.result[0].length];\\n    def json = \"{\" +\\n               \"\\\\\"name\\\\\":\\\\\"\" + name + \"\\\\\",\" +\\n               \"\\\\\"data_type\\\\\":\\\\\"\" + dataType + \"\\\\\",\" +\\n               \"\\\\\"shape\\\\\":\" + shape + \",\" +\\n               \"\\\\\"data\\\\\":\" + params.result[0] +\\n               \"}\";\\n    return json;\\n    ',\n",
    "                    \"request_body\": \"[\\\"${parameters.inputs}\\\"]\"\n",
    "             \n",
    "                 }\n",
    "                \n",
    "               \n",
    "            }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d82a8a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "connector_path_url = host+'_plugins/_ml/connectors/_create'\n",
    "register_model_path_url = host+'_plugins/_ml/models/_register'\n",
    "\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "for remote_ml_key in remote_ml.keys():\n",
    "    \n",
    "    #create connector\n",
    "    payload_1 = {\n",
    "       \"name\": remote_ml_key+\": embedding\",\n",
    "       \"description\": \"Test connector for\"+remote_ml_key+\" remote embedding model\",\n",
    "       \"version\": 1,\n",
    "       \"protocol\": \"aws_sigv4\",\n",
    "       \"credential\": {\n",
    "          \"roleArn\": \"arn:aws:iam::\"+account_id+\":role/opensearch-sagemaker-role\"\n",
    "       },\n",
    "       \"parameters\": {\n",
    "          \"region\": region,\n",
    "          \"service_name\": remote_ml_key.split(\"_\")[0]\n",
    "       },\n",
    "       \"actions\": [\n",
    "          {\n",
    "             \"action_type\": \"predict\",\n",
    "             \"method\": \"POST\",\n",
    "             \"headers\": {\n",
    "                \"content-type\": \"application/json\"\n",
    "             },\n",
    "             \"url\": remote_ml[remote_ml_key][\"endpoint_url\"],\n",
    "             \"pre_process_function\": remote_ml[remote_ml_key][\"pre_process_fun\"],\n",
    "              \"request_body\": remote_ml[remote_ml_key][\"request_body\"],\n",
    "             #\"post_process_function\": remote_ml[remote_ml_key][\"post_process_fun\"]\n",
    "          }\n",
    "       ]\n",
    "    }\n",
    "    \n",
    "\n",
    "    r_1 = requests.post(connector_path_url, auth=awsauth, json=payload_1, headers=headers)\n",
    "    remote_ml[remote_ml_key][\"connector_id\"] = json.loads(r_1.text)[\"connector_id\"]\n",
    "    \n",
    "    time.sleep(2)\n",
    "    \n",
    "    #register model\n",
    "    \n",
    "    payload_2 = { \n",
    "                \"name\": remote_ml_key,\n",
    "                \"function_name\":\"remote\",\n",
    "                \"description\": remote_ml_key+\" embeddings model\",\n",
    "                \"connector_id\": remote_ml[remote_ml_key][\"connector_id\"]\n",
    "                \n",
    "                }\n",
    "\n",
    "    r_2 = requests.post(register_model_path_url, auth=awsauth, json=payload_2, headers=headers)\n",
    "    remote_ml[remote_ml_key][\"model_id\"] = json.loads(r_2.text)[\"model_id\"]\n",
    "    \n",
    "    time.sleep(2)\n",
    "    \n",
    "    #deploy model\n",
    "    \n",
    "    deploy_model_path_url = host+'_plugins/_ml/models/'+remote_ml[remote_ml_key][\"model_id\"]+'/_deploy'\n",
    "\n",
    "    r_3 = requests.post(deploy_model_path_url, auth=awsauth, headers=headers)\n",
    "    deploy_status = json.loads(r_3.text)[\"status\"]\n",
    "    print(\"Deployment status of the \"+remote_ml_key+\" model, \"+remote_ml[remote_ml_key][\"model_id\"]+\" : \"+deploy_status)\n",
    "    \n",
    "    \n",
    "    #test model\n",
    "\n",
    "    payload_4 = {\n",
    "      \"parameters\": {\n",
    "        \"inputs\": \"hello\"\n",
    "          }\n",
    "            }\n",
    "\n",
    "    path_4 = host+'_plugins/_ml/models/'+remote_ml[remote_ml_key][\"model_id\"]+'/_predict'\n",
    "    r_4 = requests.post(path_4, auth=awsauth, json=payload_4, headers=headers)\n",
    "    print(r_4.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c4fa35",
   "metadata": {},
   "source": [
    "## Step 4: Create the OpenSearch ingest pipeline with sparse_encoding processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd49613",
   "metadata": {},
   "source": [
    "In the ingestion pipeline, you choose \"text_embedding\" processor to generate vector embeddings from \"caption\" field and store vector data in \"caption_embedding\" field of type knn_vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40babeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"_ingest/pipeline/sagemaker_sparse-ingest-pipeline\"\n",
    "url = host + path\n",
    "payload = {\n",
    "  \"description\": \"An sparse encoding ingest pipeline\",\n",
    "  \"processors\": [\n",
    "    {\n",
    "      \"sparse_encoding\": {\n",
    "        \"model_id\": remote_ml[\"sagemaker_sparse\"][\"model_id\"],\n",
    "        \"field_map\": {\n",
    "          \"caption\": \"caption_embedding\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "r = requests.put(url, auth=awsauth, json=payload, headers=headers)\n",
    "print(r.status_code)\n",
    "print(r.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e204695",
   "metadata": {},
   "source": [
    "## Step 5: Create the Sparse index with rank_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f21a9f",
   "metadata": {},
   "source": [
    "Create the K-NN index and set the pipeline created in the previous step \"nlp-ingest-pipeline\" as the default pipeline. The caption_embedding field must be mapped as a k-NN vector with 4096 dimensions matching the model dimension. \n",
    "\n",
    "For the kNN index we use **nmslib** engine with **hnsw** algorithm and **l2** spacetype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cb36eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"sagemaker_sparse-search-index\"\n",
    "url = host + path\n",
    "payload = {\n",
    "  \"settings\": {\n",
    "    \n",
    "    \"default_pipeline\": \"sagemaker_sparse-ingest-pipeline\",\n",
    "    \"number_of_shards\": 4,\n",
    "    \"number_of_replicas\": \"0\"\n",
    "  },\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"caption_embedding\": {\n",
    "        \"type\": \"rank_features\"\n",
    "      },\n",
    "      \"caption\": {\n",
    "        \"type\": \"text\"\n",
    "      },\n",
    "        \"image_s3_url\": {\n",
    "        \"type\": \"text\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "r = requests.put(url, auth=awsauth, json=payload, headers=headers)\n",
    "print(r.status_code)\n",
    "print(r.text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9ee94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"sagemaker_sparse-search-index-retail\"\n",
    "url = host + path\n",
    "payload = {\n",
    "  \"settings\": {\n",
    "    \n",
    "    \"default_pipeline\": \"sagemaker_sparse-ingest-pipeline\",\n",
    "    \"number_of_shards\": 4,\n",
    "    \"number_of_replicas\": \"0\"\n",
    "  },\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"caption_embedding\": {\n",
    "        \"type\": \"rank_features\"\n",
    "      },\n",
    "            \"image_caption\": {\n",
    "        \"type\": \"text\"\n",
    "      },\n",
    "            \"image_category\": {\n",
    "        \"type\": \"text\"\n",
    "      },\n",
    "              \"image_style\": {\n",
    "        \"type\": \"text\"\n",
    "      },\n",
    "                \"image_price\": {\n",
    "        \"type\": \"double\"\n",
    "      },\n",
    "                  \"image_gender\": {\n",
    "        \"type\": \"text\"\n",
    "      },\n",
    "                     \"image_stock\": {\n",
    "        \"type\": \"integer\"\n",
    "      },\n",
    "      \"caption\": {\n",
    "        \"type\": \"text\"\n",
    "      },\n",
    "        \"image_s3_url\": {\n",
    "        \"type\": \"text\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "r = requests.put(url, auth=awsauth, json=payload, headers=headers)\n",
    "print(r.status_code)\n",
    "print(r.text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3269d744",
   "metadata": {},
   "source": [
    "## Step 6: Prepare the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9815d45",
   "metadata": {},
   "source": [
    "Download the Amazon Bekerley dataset from S3 and pre-process in such a way that you get the image properties in a dataframe\n",
    "\n",
    "For simplicity we use only 1655 sample images from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ed48ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "#meta = pd.read_json(\"s3://amazon-berkeley-objects/listings/metadata/listings_0.json.gz\", lines=True)\n",
    "\n",
    "appended_data = []\n",
    "\n",
    "for character in string.digits[0:]+string.ascii_lowercase:\n",
    "    if(character == 'g'):\n",
    "        break\n",
    "    meta = pd.read_json(\"s3://amazon-berkeley-objects/listings/metadata/listings_\"+character+\".json.gz\", lines=True)\n",
    "    appended_data.append(meta)\n",
    "\n",
    "appended_data_frame = pd.concat(appended_data)\n",
    "\n",
    "appended_data_frame.shape\n",
    "meta = appended_data_frame\n",
    "def func_(x):\n",
    "    us_texts = [item[\"value\"] for item in x if item[\"language_tag\"] == \"en_US\"]\n",
    "    return us_texts[0] if us_texts else None\n",
    " \n",
    "meta = meta.assign(item_name_in_en_us=meta.item_name.apply(func_))\n",
    "meta = meta[~meta.item_name_in_en_us.isna()][[\"item_id\", \"item_name_in_en_us\", \"main_image_id\"]]\n",
    "print(f\"#products with US English title: {len(meta)}\")\n",
    "meta.head()\n",
    "\n",
    "image_meta = pd.read_csv(\"s3://amazon-berkeley-objects/images/metadata/images.csv.gz\")\n",
    "dataset = meta.merge(image_meta, left_on=\"main_image_id\", right_on=\"image_id\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72bcc58",
   "metadata": {},
   "source": [
    "## Step 7: Ingest the prepared data into OpenSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fb015a",
   "metadata": {},
   "source": [
    "We ingest only the captions and the image urls of the images into the opensearch index\n",
    "\n",
    "This step takes approcimately 10 minutes to load the data into opensearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a649f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "from alive_progress import alive_bar\n",
    "port = 443\n",
    "\n",
    "\n",
    "host = 'https://'+OpenSearchDomainEndpoint+'/'\n",
    "service = 'es'\n",
    "credentials = boto3.Session().get_credentials()\n",
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key, region, service, session_token=credentials.token)\n",
    "headers = { \"Content-Type\": \"application/json\"}\n",
    "client = OpenSearch(\n",
    "    hosts = [{'host': OpenSearchDomainEndpoint, 'port': 443}],\n",
    "    http_auth = awsauth,\n",
    "    use_ssl = True,\n",
    "    #verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection\n",
    ")\n",
    "\n",
    "cnt = 0\n",
    "batch = 0\n",
    "action = json.dumps({ \"index\": { \"_index\": \"sagemaker_sparse-search-index\" } })\n",
    "body_ = ''\n",
    "body_1 = ''\n",
    "\n",
    "\n",
    "with alive_bar(len(dataset), force_tty = True) as bar:\n",
    "    for index, row in (dataset.iterrows()):\n",
    "        if(row['path'] == '87/874f86c4.jpg' or row['path'] ==  'b5/b5319e00.jpg'):\n",
    "            continue\n",
    "\n",
    "        payload = {}\n",
    "        payload['image_s3_url'] = \"https://amazon-berkeley-objects.s3.amazonaws.com/images/small/\"+row['path']\n",
    "        payload['caption'] = row['item_name_in_en_us']\n",
    "        body_ = body_ + action + \"\\n\" + json.dumps(payload) + \"\\n\"\n",
    "        body_1 = body_1 + action_1 + \"\\n\" + json.dumps(payload) + \"\\n\"\n",
    "        cnt = cnt+1\n",
    "\n",
    "\n",
    "        if(cnt == 100):\n",
    "            \n",
    "            response = client.bulk(\n",
    "                                index = 'sagemaker_sparse-search-index',\n",
    "                                 body = body_)\n",
    "             #r = requests.post(url, auth=awsauth, json=body_+\"\\n\", headers=headers)\n",
    "            cnt = 0\n",
    "            batch = batch +1\n",
    "            body_ = ''\n",
    "            body_1 = ''\n",
    "        \n",
    "        bar()\n",
    "print(\"Total Bulk batches completed: \"+str(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b535c1c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "from alive_progress import alive_bar\n",
    "port = 443\n",
    "from ruamel.yaml import YAML\n",
    "\n",
    "yaml = YAML()\n",
    "input_file = 'products.yaml'\n",
    "\n",
    "items_ = yaml.load(open(input_file))\n",
    "\n",
    "host = 'https://'+OpenSearchDomainEndpoint+'/'\n",
    "service = 'es'\n",
    "credentials = boto3.Session().get_credentials()\n",
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key, region, service, session_token=credentials.token)\n",
    "headers = { \"Content-Type\": \"application/json\"}\n",
    "client = OpenSearch(\n",
    "    hosts = [{'host': OpenSearchDomainEndpoint, 'port': 443}],\n",
    "    http_auth = awsauth,\n",
    "    use_ssl = True,\n",
    "    #verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection\n",
    ")\n",
    "\n",
    "cnt = 0\n",
    "with alive_bar(len(items_), force_tty = True) as bar:\n",
    "    for item in items_:\n",
    "        cnt = cnt +1\n",
    "#         if(cnt<1430):\n",
    "#             print(\"skipping\")\n",
    "#             continue\n",
    "        if(cnt%100 == 0):\n",
    "            host = 'https://'+OpenSearchDomainEndpoint+'/'\n",
    "            service = 'es'\n",
    "            credentials = boto3.Session().get_credentials()\n",
    "            awsauth = AWS4Auth(credentials.access_key, credentials.secret_key, region, service, session_token=credentials.token)\n",
    "            headers = { \"Content-Type\": \"application/json\"}\n",
    "            client = OpenSearch(\n",
    "                hosts = [{'host': OpenSearchDomainEndpoint, 'port': 443}],\n",
    "                http_auth = awsauth,\n",
    "                use_ssl = True,\n",
    "                #verify_certs = True,\n",
    "                connection_class = RequestsHttpConnection\n",
    "            )\n",
    "            \n",
    "        payload = {}\n",
    "        payload['image_s3_url'] = \"https://retail-demo-store-us-east-1.s3.amazonaws.com/images/\"+item[\"category\"]+\"/\"+item[\"image\"]\n",
    "        payload['caption'] = item['description']\n",
    "        payload['image_price'] = item['price']\n",
    "        if('style' in item):\n",
    "            payload['image_style'] = item['style']\n",
    "        payload['image_category'] = item['category']\n",
    "        if('current_stock' in item):\n",
    "            payload['image_current_stock'] = item['current_stock']\n",
    "        if('gender_affinity' in item):\n",
    "            payload['image_gender'] = item['gender_affinity']\n",
    "        payload['image_caption'] = item['name']\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        response = client.index(\n",
    "            index = 'sagemaker_sparse-search-index-retail',\n",
    "            body = payload\n",
    "        )\n",
    "      \n",
    "        \n",
    "        \n",
    "        bar()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607dfffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optional code for calling the model directly from local\n",
    "\n",
    "import json\n",
    "\n",
    "from ts.torch_handler.base_handler import BaseHandler\n",
    "import torch\n",
    "import os\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from sagemaker_inference import logging\n",
    "from ts.handler_utils.micro_batching import MicroBatching\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file='test/tokenizer.json')\n",
    "device = torch.device(\"cuda:\" + str(properties.get(\"gpu_id\")) if torch.cuda.is_available() else \"cpu\")\n",
    "model = torch.jit.load('test/opensearch-neural-sparse-encoding-v1.pt', map_location=device)\n",
    "\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "from alive_progress import alive_bar\n",
    "port = 443\n",
    "\n",
    "\n",
    "host = 'https://'+OpenSearchDomainEndpoint+'/'\n",
    "service = 'es'\n",
    "credentials = boto3.Session().get_credentials()\n",
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key, region, service, session_token=credentials.token)\n",
    "headers = { \"Content-Type\": \"application/json\"}\n",
    "client = OpenSearch(\n",
    "    hosts = [{'host': OpenSearchDomainEndpoint, 'port': 443}],\n",
    "    http_auth = awsauth,\n",
    "    use_ssl = True,\n",
    "    #verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection\n",
    ")\n",
    "\n",
    "cnt = 0\n",
    "batch__ = 0\n",
    "action = json.dumps({ \"index\": { \"_index\": \"sagemaker_sparse-search-index\" } })\n",
    "body_ = ''\n",
    "body_1 = ''\n",
    "\n",
    "\n",
    "with alive_bar(len(dataset), force_tty = True) as bar:\n",
    "    for index, row in (dataset.iterrows()):\n",
    "        cnt = cnt+1\n",
    "        if(row['path'] == '87/874f86c4.jpg' or row['path'] ==  'b5/b5319e00.jpg' or cnt<=26000):\n",
    "            continue\n",
    "        if(batch__%25 == 0):\n",
    "            host = 'https://'+OpenSearchDomainEndpoint+'/'\n",
    "            service = 'es'\n",
    "            credentials = boto3.Session().get_credentials()\n",
    "            awsauth = AWS4Auth(credentials.access_key, credentials.secret_key, region, service, session_token=credentials.token)\n",
    "            client = OpenSearch(\n",
    "            hosts = [{'host': OpenSearchDomainEndpoint, 'port': 443}],\n",
    "            http_auth = awsauth,\n",
    "            use_ssl = True,\n",
    "            #verify_certs = True,\n",
    "            connection_class = RequestsHttpConnection\n",
    "        )\n",
    "        payload = {}\n",
    "        payload['image_s3_url'] = \"https://amazon-berkeley-objects.s3.amazonaws.com/images/small/\"+row['path']\n",
    "        payload['caption'] = row['item_name_in_en_us']\n",
    "        #cnt = cnt+1\n",
    "        ############################\n",
    "        inputSentence = [row['item_name_in_en_us']]\n",
    "        input_data = tokenizer(inputSentence, return_tensors=\"pt\", padding=True, add_special_tokens=True,\n",
    "                                            max_length=256,\n",
    "                                            truncation=\"longest_first\", return_attention_mask=True)\n",
    "                #logger.info(inputSentence)\n",
    "        input_data = input_data.to(device)\n",
    "        input_data\n",
    "        predictions = model(input_data)\n",
    "        prediction = {\"pred\": predictions, \"batch_l\": [1]}\n",
    "        batch_idx = prediction[\"batch_l\"]\n",
    "        prediction = prediction[\"pred\"]\n",
    "        batch = len(prediction[\"output\"])\n",
    "        output = []\n",
    "        tensor = prediction[\"output\"]\n",
    "        for i in range(batch):\n",
    "            tokenWeights = {}\n",
    "            nonzero_indices = tensor[i].nonzero().squeeze()  # Get indices of nonzero elements for the ith batch\n",
    "            for idx in nonzero_indices:\n",
    "                if tensor[i][idx] > 0:\n",
    "                    tokenWeights[tokenizer.decode([idx.item()])] = float(tensor[i][idx])\n",
    "            output.append(tokenWeights)\n",
    "        outputs = []\n",
    "        index = 0\n",
    "        #print(\"batch info\", len(output), batch_idx)\n",
    "        for b in batch_idx:\n",
    "            #print(\"index:\", index, b, len(output[index:index + b]))\n",
    "            outputs.append(output[index:index + b])\n",
    "            index += b\n",
    "        ##################################\n",
    "        payload['caption_embedding'] = outputs[0][0]\n",
    "        body_ = body_ + action + \"\\n\" + json.dumps(payload) + \"\\n\"\n",
    "        \n",
    "        if(cnt == 1000):\n",
    "            \n",
    "            response = client.bulk(\n",
    "                                index = 'sagemaker_sparse-search-index',\n",
    "                                 body = body_)\n",
    "            #r = requests.post(url, auth=awsauth, json=body_+\"\\n\", headers=headers)\n",
    "            cnt = 0\n",
    "            batch__ = batch__ +1\n",
    "            #body_ = ''\n",
    "            body_1 = ''\n",
    "            \n",
    "        \n",
    "        bar()\n",
    "print(\"Total Bulk batches completed: \"+str(batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f39448",
   "metadata": {},
   "source": [
    "## Step 8: Update the environment variables of lambda\n",
    "\n",
    "Here, we pass the OpenSearch endpoint, AWS region and OpenSearch model identifier to Lambda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c557766",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_client = boto3.client('lambda')\n",
    "\n",
    "response = lambda_client.update_function_configuration(\n",
    "            FunctionName=lambdaFunction,\n",
    "            Environment={\n",
    "                'Variables': {\n",
    "                    'DOMAIN_ENDPOINT': OpenSearchDomainEndpoint,\n",
    "                    'REGION':region,\n",
    "                    'SAGEMAKER_MODEL_ID':sagemaker_model_id,\n",
    "                    'BEDROCK_TEXT_MODEL_ID':bedrock_text_model_id,\n",
    "                    'BEDROCK_MULTIMODAL_MODEL_ID':bedrock_multimodal_model_id,\n",
    "                    \n",
    "                }\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18947159",
   "metadata": {},
   "source": [
    "## Step 9: Create the Lambda URL\n",
    "\n",
    "Here we create external Lambda URL for lambda function to be called from the outside world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a97269a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ = boto3.client('lambda')\n",
    "\n",
    "\n",
    "response_ = lambda_.add_permission(\n",
    "FunctionName=lambdaFunction,\n",
    "StatementId=lambdaFunction+'_permissions',\n",
    "Action=\"lambda:InvokeFunctionUrl\",\n",
    "Principal=account_id,\n",
    "FunctionUrlAuthType='AWS_IAM')\n",
    "\n",
    "\n",
    "response = lambda_.create_function_url_config(\n",
    "FunctionName=lambdaFunction,\n",
    "AuthType='AWS_IAM',\n",
    "Cors={\n",
    "    'AllowCredentials': True,\n",
    "\n",
    "    'AllowMethods':[\"*\"],\n",
    "    'AllowOrigins': [\"*\"]\n",
    "\n",
    "},\n",
    "InvokeMode='RESPONSE_STREAM'\n",
    ")\n",
    "\n",
    "query_invoke_URL = response['FunctionUrl']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1154fad",
   "metadata": {},
   "source": [
    "## Step 10: Host the Hybrid Search application in EC2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b857dea",
   "metadata": {},
   "source": [
    "## Notice\n",
    "\n",
    "To ensure security access to the provisioned resources, we use EC2 security group to limit access scope. Before you go into the final step, you need to add your current **PUBLIC IP** address to the ec2 security group so that you are able to access the web application (chat interface) that you are going to host in the next step.\n",
    "\n",
    "<h3 style=\"color:red;\"><U>Warning</U></h3>\n",
    "<h4>Without doing the below steps, you will not be able to proceed further.</h4>\n",
    "\n",
    "<div>\n",
    "    <h3 style=\"color:red;\"><U>Enter your IP address </U></h3>\n",
    "    <h4> STEP 1. Get your IP address <span style=\"display:inline;color:blue\"><a href = \"https://ipinfo.io/ip \">HERE</a></span>. If you are connecting with VPN, we recommend you disconnect VPN first.</h4>\n",
    "</div>\n",
    "\n",
    "<h4>STEP 2. Run the below cell </h4>\n",
    "<h4>STEP 3. Paste the IP address in the input box that prompts you to enter your IP</h4>\n",
    "<h4>STEP 4. Press ENTER</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddb056a",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_ip = (input(\"Enter your IP : \")).split(\".\")\n",
    "my_ip.pop()\n",
    "IP = \".\".join(my_ip)+\".0/24\"\n",
    "\n",
    "port_protocol = {443:'HTTPS',80:'HTTP',8501:'streamlit'}\n",
    "\n",
    "IpPermissions = []\n",
    "\n",
    "for port in port_protocol.keys():\n",
    "     IpPermissions.append({\n",
    "            'FromPort': port,\n",
    "            'IpProtocol': 'tcp',\n",
    "            'IpRanges': [\n",
    "                {\n",
    "                    'CidrIp': IP,\n",
    "                    'Description': port_protocol[port]+' access',\n",
    "                },\n",
    "            ],\n",
    "            'ToPort': port,\n",
    "        })\n",
    "\n",
    "IpPermissions\n",
    "\n",
    "for output in cfn_outputs:\n",
    "    if('securitygroupid' in output['OutputKey'].lower()):\n",
    "        sg_id = output['OutputValue']\n",
    "        \n",
    "#sg_id = 'sg-0e0d72baa90696638'\n",
    "\n",
    "ec2_ = boto3.client('ec2')        \n",
    "\n",
    "response = ec2_.authorize_security_group_ingress(\n",
    "    GroupId=sg_id,\n",
    "    IpPermissions=IpPermissions,\n",
    ")\n",
    "\n",
    "print(\"\\nIngress rules added for the security group, ports:protocol - \"+json.dumps(port_protocol)+\" with my ip - \"+IP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047e4328",
   "metadata": {},
   "source": [
    "Finally, We are ready to host our conversational search application, here we perform the following steps, Steps 2-5 are achieved by executing the terminal commands in the ec2 instance using a SSM client.\n",
    "1. Update the web application code files with lambda url (in [api.py](https://github.com/aws-samples/semantic-search-with-amazon-opensearch/blob/main/generative-ai/Module_1_Build_Conversational_Search/webapp/api.py)) and s3 bucket name (in [app.py](https://github.com/aws-samples/semantic-search-with-amazon-opensearch/blob/main/generative-ai/Module_1_Build_Conversational_Search/webapp/app.py))\n",
    "2. Archieve the application files and push to the configured s3 bucket.\n",
    "3. Download the application (.zip) from s3 bucket into ec2 instance (/home/ec2-user/), and uncompress it.\n",
    "4. We install the streamlit and boto3 dependencies inside a virtual environment inside the ec2 instance.\n",
    "5. Start the streamlit application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc79d845",
   "metadata": {},
   "outputs": [],
   "source": [
    "#modify the code files with lambda url and s3 bucket names\n",
    "query_invoke_URL_cmd = query_invoke_URL.replace(\"/\",\"\\/\")\n",
    "\n",
    "with io.capture_output() as captured:\n",
    "    #Update the webapp files to include the s3 bucket name and the LambdaURL\n",
    "    !sed -i 's/API_URL_TO_BE_REPLACED/{query_invoke_URL_cmd}/g' webapp/api.py\n",
    "    #Push the WebAPP code artefacts to s3\n",
    "    !cd webapp && zip -r webapp.zip *\n",
    "    !aws s3 cp webapp/webapp.zip s3://$s3_bucket\n",
    "        \n",
    "#Get the Ec2 instance ID which is already deployed\n",
    "response = cfn.describe_stack_resources(\n",
    "    StackName=stackname\n",
    ")\n",
    "for resource in response['StackResources']:\n",
    "    if(resource['ResourceType'] == 'AWS::EC2::Instance'):\n",
    "        ec2_instance_id = resource['PhysicalResourceId']\n",
    "   \n",
    "ec2_instance_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b1351e",
   "metadata": {},
   "source": [
    "Copy the URL that will be generated after running the next cell and open the URL in your web browser to start using the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e2339b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to execute commands in ec2 terminal\n",
    "def execute_commands_on_linux_instances(client, commands):\n",
    "    resp = client.send_command(\n",
    "        DocumentName=\"AWS-RunShellScript\", # One of AWS' preconfigured documents\n",
    "        Parameters={'commands': commands},\n",
    "        InstanceIds=[ec2_instance_id],\n",
    "    )\n",
    "    return resp['Command']['CommandId']\n",
    "\n",
    "ssm_client = boto3.client('ssm') \n",
    "\n",
    "commands = [\n",
    "            'aws s3 cp s3://'+s3_bucket+'/webapp.zip /home/ec2-user/',\n",
    "            'unzip -o /home/ec2-user/webapp.zip -d /home/ec2-user/'  ,  \n",
    "            'sudo chmod -R 0777 /home/ec2-user/',\n",
    "            'python3 -m venv /home/ec2-user/.myenv',\n",
    "            'source /home/ec2-user/.myenv/bin/activate',\n",
    "            'pip install streamlit',\n",
    "            'pip install boto3',\n",
    "    \n",
    "            #start the web applicaiton\n",
    "            'streamlit run /home/ec2-user/app.py',\n",
    "            ]\n",
    "\n",
    "command_id = execute_commands_on_linux_instances(ssm_client, commands)\n",
    "\n",
    "ec2_ = boto3.client('ec2')\n",
    "response = ec2_.describe_instances(\n",
    "    InstanceIds=[ec2_instance_id]\n",
    ")\n",
    "public_ip = response['Reservations'][0]['Instances'][0]['PublicIpAddress']\n",
    "print(\"Please wait while the application is being hosted . . .\")\n",
    "time.sleep(10)\n",
    "print(\"\\nApplication hosted successfully\")\n",
    "print(\"\\nClick the below URL to open the application. It may take up to a minute or two to start the application, Please keep refreshing the page if you are seeing connection error.\\n\")\n",
    "print('http://'+public_ip+\":8501\")\n",
    "#print(\"\\nCheck the below video on how to interact with the application\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
